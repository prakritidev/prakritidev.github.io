{"AWS-Associate-Architect-Notes/2018-AWS-GuideLines":{"title":"\"2018 AWS GuideLines\"","links":[],"tags":["AWS","Exam","certificate"],"content":"2018 AWS exams guidelines\nDomain 1: Design Resilient Architectures 1.1 Choose reliable/resilient storage. 1.2 Determine how to design decoupling mechanisms using AWS services. 1.3 Determine how to design a multi-tier architecture solution. 1.4 Determine how to design high availability and/or fault tolerant architectures. \nDomain 2: Define Performant Architectures 2.1 Choose performant storage and databases. 2.2 Apply caching to improve performance. 2.3 Design solutions for elasticity and scalability. \nDomain 3: Specify Secure Applications and Architectures 3.1 Determine how to secure application tiers. Determine how to secure data. 3.3 Define the networking infrastructure for a single VPC application.\n Domain 4: Design Cost-Optimized Architectures 4.1 Determine how to design cost-optimized storage. 4.2 Determine how to design cost-optimized compute. \nDomain 5: Define Operationally-Excellent Architectures 5.1 Choose design features in solutions that enable operational excellence.\nDomain 1: Design Resilient Architectures 34% \nDomain 2: Define Performant Architectures 24% \nDomain 3: Specify Secure Applications and Architectures 26% \nDomain 4: Design Cost-Optimized Architectures 10% \nDomain 5: Define Operationally Excellent Architectures 6% TOTAL 100%"},"AWS-Associate-Architect-Notes/AWS---EC2-AND-EBS":{"title":"\"AWS - EC2 AND EBS\"","links":[],"tags":["AWS","ec2","ebs","networking"],"content":"EC2 is a primary web service component of AWS that provide resizable compute power. \nInstance type families: \n\n\nC4 compute \n\n\nR3 RAM\n\n\nI2 I/O\n\n\nG2 GPU instances\n\n\nAMI (Amazon Machines Image)\n\n\nIt’s an operating system image , that might or might not be customised this is installed on EC2 Instance \n\n\nLinux AMI\n\n\nWindows AMI\n\n\nAMI’s for Deep Learning \n\n\nBased on X86 arch OS\n\n\nFour Sources for finding AMI on AWS \n\n\nPublished by AWS \n\n\nFrom Marketplace on AWS \n\n\nGenerated from Existing Instances \n\n\nUpload virtual servers (virtual machine, etc)\n\n\nSecurely using an Instance (Once launched it can be accessed over the internet)\n\n\nDNS\n\n\nAws provides a DNS name to the EC2 instance\n\n\nUser can’t change this on its own \n\n\nCan’t be transfer from one instance to another\n\n\nPublic IP\n\n\nAWS asign a public IP address to the Instance \n\n\nThis Ip is assigned from the reserved addresses from AWS \n\n\nElastic IP\n\n\nIts an address that we reserve independently and associate it with our EC2 Instance \n\n\nIt persists until user releases it\n\n\nIt can be transferred on the time of failure of the instance \n\n\nInitial Access of Launched EC2 \n\n\nAWS provides a key-pair, public key is provided to the user and private key is held by AWS \n\n\nVirtual Firewall Protection \n\n\nUsing Security groups \n\n\nSecurity Groups for EC2 will give the control on outgoing traffic \n\n\nFor VPC is controls outgoing/Incoming traffic  \n\n\nInstance can have more than one security groups attached \n\n\nInstance must attach to one security group \n\n\nDefault is Deny \n\n\nPort : allowing connection on specific port, for HTTP it’s 80. \n\n\nProtocol: HTTP\n\n\nSource/Destination : Identifies other end of the communication\n\n\nCIDR block: x.x.x.x/x, It will allow specific range of IP\n\n\nSecurity Group: Includes any instance the is associates with given group. Helps Preventing coupling od security coups associated with IP addresses. (I did not understand the meaning of this.)\n\n\nApplied at Instance level (better for security purposes)\n\n\nLife Cycle of Instance \n\n\nBootstrapping: Pre-configuration of machine and install software on the fly \n\n\nThe bootstrap script is stored with the string value “User Data”. \n\n\nCan also import VM machine into EC2 \n\n\nTags are available for tagging EC2, easy to grouping when you have large number of EC2 \n\n\nKeys, values pair to store Tags \n\n\nProject : RecSys\n\n\nDevelopment : Production \n\n\nBilling Code : 110011 \n\n\nMonitoring EC2 \n\n\nCloudWatch service \n\n\nPricing Options \n\n\nOn-Demand Instances \n\n\nPer hour Pricing\n\n\nEBS (Elastic Block Storage)\n\n\nBasics \n\n\nAWS provides a persistent block-level storage volumes  \n\n\nProtects from component failure, high availability and durability \n\n\nCan be attached to one Instance at a time \n\n\nData replicated within same region for fault tolerance \n\n\nTypes \n\n\nMagnetic \n\n\nLowest performance, lower price, \n\n\nRange form 1 GB to 1 TB \n\n\n100 average IOPS, can Burst upto 100s \n\n\nBill based on data space provisioned \n\n\nGeneral purpose SSD\n\n\nUsed where high performance is not required. \n\n\nIf IOPS are not used then they are stored as a IO credit and can be used when you need heavy lifting which can \n\n\nProvisioned IOPS SSD \n\n\nFor I/O intensive workloads \n\n\n20,000 IOPS\n\n\nEBS-Optimised Instances \n\n\nBetter optimised \n\n\nYou pay additional per hour price \n\n\nData Protection\nBackups/Recovery (Snapshot)\n\nOn EBS backups can be done by taking snapshots we can put a scheduler for regular backups and save them to S3 \nOnly changes in the snapshot what we pay for \nWe can’t use them as like our S3 buckets but we can use them to restore data \nWe can create volume from snapshots. Data is loaded lazily. \nEBS persists after the instance termination so the data is stored in root volume and can be used with other instance \nIf instance set on termination on deletion then data on EBS will be lost \nEBS can be encrypted using AWS key Management and it’s transparent\n"},"AWS-Associate-Architect-Notes/Amazon-ElastiCache":{"title":"\"Amazon ElastiCache\"","links":[],"tags":["AWS","redis","Memcached","cache","cloud"],"content":"Amazon ElastiCache \nAmazon ElastiCache is a web service that simplifies the setup and management of distributed in-memory caching environments. This service makes it easy and cost effective to provide a high-performance and scalable caching solution for your cloud applications. You can use Amazon ElastiCache in your applications to speed the deployment of cache clusters and reduce the administration required for a distributed cache environment. \nData Access Patterns \nRetrieving a flat key from an in-memory cache will always be faster than the most optimized database query. You should evaluate the access pattern of the data before you decide to store it in cache. \nCache Engines\nAmazon ElastiCache allows you to quickly deploy clusters of two different types of popular cache engines: Memcached and Redis. At a high level, Memcached and Redis may seem similar, but they support a variety of different use cases and provide different functionality."},"AWS-Associate-Architect-Notes/DNS-and-Route53":{"title":"\"DNS and Route53\"","links":[],"tags":["AWS","route53","routing","dns","recordTypes","basics"],"content":"Domain Name System (DNS) and Amazon Route 53 (Need to go through again)\nDomain Name System (DNS) \nAmazon Route 53 is an authoritative DNS system. An authoritative DNS system provides an update mechanism that developers use to manage their public DNS names. It then answers DNS queries, translating domain names into IP addresses so that computers can communicate with each other. \nConcepts \nTop-Level Domain (TLD) : .com, .net, .org, .gov, .edu, and .io. TLDs are at the top of the hierarchy in terms of domain names \nIP Addresses : An IP address is a network addressable location. Each IP address must be unique within its network. For public websites, this network is the entire Internet. \nHosts : Within a domain, the domain owner can define individual hosts, which refer to separate computers or services accessible through a domain. For instance, most domain owners make their web servers accessible through the base domain (example.com) and also through the host definition www (as in www.example.com). \nSubdomains : DNS works in a hierarchal manner and allows a large domain to be partitioned or extended into multiple subdomains. TLDs can have many subdomains under them. For instance, zappos.com and audible.com are both subdomains of the .com TLD (although they are typically just called domains). The zappos or audible portion can be referred to as an SLD. \nFully Qualified Domain Name (FQDN) : Domain locations in a DNS can be relative to one another and, as such, can be somewhat ambiguous. A Fully Qualified Domain Name (FQDN), also referred to as an absolute domain name, specifies a domain’s location in relation to the absolute root of the DNS. \nThis means that the FQDN specifies each parent domain including the TLD. \nA proper FQDN ends with a dot, indicating the root of the DNS hierarchy. For example, mail .amazon.com is an FQDN. Sometimes, software that calls for an FQDN does not require the ending dot, but it is required to conform to ICANN standards. \nZone Files \nA zone file is a simple text file that contains the mappings between domain names and IP addresses. This is how a DNS server finally identifies which IP address should be contacted when a user requests a certain domain name. \nZone files reside in name servers and generally define the resources available under a specific domain, or the place where one can go to get that information. \nMore About Zone Files \nZone files are the way that name servers store information about the domains they know. The more zone files that a name server has, the more requests it will be able to answer authoritatively. Most requests to the average name server, however, are for domains that are not in the local zone file. \nRecord Types\n\n\nStart of Authority (SOA) Record : A Start of Authority (SOA) record is mandatory in all zone files, and it identifies the base DNS information about the domain. Each zone contains a single SOA record. \n\n\nSOA records following data \n\n\nThe name of the DNS server for that zone \n\n\nAdministrator of the zone \n\n\nCurrent version of the data file \n\n\nNumber of seconds that a secondary name server should wait before checking for updates \n\n\nNumber of seconds that a secondary name server should wait before retrying a failed zone transfer \n\n\nA and AAAA : Both types of address records map a host to an IP address. The A record is used to map a host to an IPv4 IP address, while AAAA records are used to map a host to an IPv6 address. \n\n\nA Canonical Name (CNAME) : Canonical Name (CNAME) record is a type of resource record in the DNS that defines an alias for the CNAME for your server (the domain name defined in an A or AAAA record). \n\n\nA Name Server (NS) : Name Server (NS) records are used by TLD servers to direct traffic to the DNS server that contains the authoritative DNS records. \n\n\nSender Policy Framework (SPF) : Sender Policy Framework (SPF) records are used by mail servers to combat spam. An SPF record tells a mail server what IP addresses are authorized to send an email from your domain name. For example, if you wanted to ensure that only your mail server sends emails from your company’s domain, such as example.com, you would create an SPF record with the IP address of your mail server. That way, an email sent from your domain, such as marketing@example.com, would need to have an originating IP address of your company mail server in order to be accepted. This prevents people from spoofing emails from your domain name. \n\n\nService (SRV): A Service (SRV) record is a specification of data in the DNS defining the location (the host name and port number) of servers for specified services. The idea behind SRV is that, given a domain name (for example, example.com) and a service name (for example, web [HTTP], which runs on a protocol [TCP]), a DNS query may be issued to find the host name that provides such a service for the domain, which may or may not be within the domain. \n\n\nAmazon Route 53 Overview \nAmazon Route 53 is a highly available and scalable cloud DNS web service that is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications. \nDomain registration—Amazon Route 53 lets you register domain names, such as example.com. \nDNS service—Amazon Route 53 translates friendly domain names like www.example.com into IP addresses like 192.0.2.1. Amazon Route 53 responds to DNS queries using a global network of authoritative DNS servers, which reduces latency. To comply with DNS standards, responses sent over User Datagram Protocol (UDP) are limited to 512 bytes in size. Responses exceeding 512 bytes are truncated, and the resolver must re-issue the request over TCP. \nHealth checking—Amazon Route 53 sends automated requests over the Internet to your application to verify that it’s reachable, available, and functional. \nHosted Zones: A hosted zone is a collection of resource record sets hosted by Amazon Route 53. Like a traditional DNS zone file, a hosted zone represents resource record sets that are managed together under a single domain name. Each hosted zone has its own metadata and configuration information. \nRouting policy \nSimple: This is the default routing policy when you create a new resource. Use a simple routing policy when you have a single resource that performs a given function for your domain (for example, one web server that serves content for the example.com website. \nWeighted: With weighted DNS, you can associate multiple resources (such as Amazon Elastic Compute Cloud [Amazon EC2] instances or Elastic Load Balancing load balancers) with a single DNS name \nLatency-Based : Latency-based routing allows you to route your traffic based on the lowest network latency for your end user (for example, using the AWS region that will give them the fastest response time). \nFailover: Use a failover routing policy to configure active-passive failover, in which one resource takes all the traffic when it’s available and the other resource takes all the traffic when the first resource isn’t available. Note that you can’t create failover resource record sets for private hosted zones. \nGeolocation : Route user based on their geolocation and set default resource if the user is pinging form unknown location. You cannot create two geolocation resource record sets that specify the same geographic location. You also cannot create geolocation resource record sets that have the same values for “Name” and “Type” as the “Name” and “Type” of non-geolocation resource record sets. \nHealth Checking: Amazon Route 53 health checks monitor the health of your resources such as web servers and email servers. You can configure Amazon CloudWatch alarms for your health checks so that you receive notification when a resource becomes unavailable. You can also configure Amazon Route 53 to route Internet traffic away from resources that are unavailable. \nAmazon Route 53 Enables Resiliency \nConcepts to build an application that is highly available and resilient to failures. \n\nElastic Load Balancing load balancer is set up with cross-zone load balancing and connection draining. This distributes the load evenly across all instances in all Availability Zones, and it ensures requests in flight are fully served before an Amazon EC2 instance is disconnected from an Elastic Load Balancing load balancer for any reason. \nEach Elastic Load Balancing load balancer also has an Amazon Route 53 health check associated with it to ensure that requests are routed only to load balancers that have healthy Amazon EC2 instances \nThe application’s failover environment (for example, fail.domain.com) has an Amazon Route 53 alias record that points to an Amazon CloudFront distribution of an Amazon S3 bucket hosting a static version of the application \nThe application is deployed in multiple AWS regions, protecting it from a regional outage\n"},"AWS-Associate-Architect-Notes/Database(RDS)-and-Redshift":{"title":"\"Database (RDS) and Redshift\"","links":[],"tags":["AWS","database","RDS","Redshift","dynamoDb","Aurora"],"content":"AMAZON RDS\nAmazon provides RDS service where AWS do everything on their own. Scaling, replicating, etc. We don’t have to do anything or we can put everything on EC2 and do everything on by own. For licensed databases like Oracle and Microsoft, you have to Bring Your Own License (BYOL) \nBackup and recovery\nAmazon RDS provides two mechanisms for backing up the database: automated backups and manual snapshots. By using a combination of both techniques, you can design a backup recovery model to protect your application data.\nRPO\nRPO is defined as the maximum period of data loss that is acceptable in the event of a failure or incident. For example, many systems back up transaction logs every 15 minutes to allow them to minimize data loss in the event of an accidental deletion or hardware failure. \nRTO\nRTO is defined as the maximum amount of downtime that is permitted to recover from backup and to resume processing. For large databases in particular, it can take hours to restore from a full backup. In the event of a hardware failure, you can reduce your RTO to minutes by failing over to a secondary node. You should create a recovery plan that, at a minimum, lets you recover from a recent backup. \nAutomated Backups\nOne day of backups will be retained by default, but you can modify the retention period up to a maximum of 35 days \nManual DB Snapshots \nUnlike automated snapshots that are deleted after the retention period, manual DB snapshots are kept until you explicitly delete them with the Amazon RDS console or the DeleteDBSnapshot action. \nSecurity\nDeploy RDS in VPC with private subnet that limits network access to the DB Instance**.** \nAmazon Redshift\nAmazon Redshift is a fast, powerful, fully managed, petabyte-scale data warehouse service in the cloud. \nClusters and Nodes\nThe key component of an Amazon Redshift data warehouse is a cluster. A cluster is composed of a leader node and one or more compute nodes. The client application interacts directly only with the leader node, and the compute nodes are transparent to external applications. \nTable Design \n\n\nData Types \n\n\nAll data types are available \n\n\nAdditional columns can be added to a table using the ALTER TABLE command; however, existing columns cannot be modified\n\n\nCompression Encoding \n\n\nOne of the key performance optimisations used by Amazon Redshift is data compression. When loading data for the first time into an empty table, Amazon Redshift will automatically sample your data and select the best compression scheme for each column\n\n\nDistributed Strategy \n\n\nThe goal in selecting a table distribution style is to minimize the impact of the redistribution step by putting the data where it needs to be before the query is performed. \n\n\nQuerying Data \n\n\nAllows you to write standard SQL query on your tables . \n\n\nSecurity \n\n\nFirst layer is IAM Policies \n\n\nPrivate subnet \n\n\nAmazon DynamoDB\nAmazon DynamoDB is designed to simplify database and cluster management, provide consistently high levels of performance, simplify scalability tasks, and improve reliability with automatic replication. Developers can create a table in Amazon DynamoDB and write an unlimited number of items with consistent latency. \nTo maximize Amazon DynamoDB throughput, create tables with a partition key that has a large number of distinct values and ensure that the values are requested fairly uniformly. Adding a random element that can be calculated or hashed is one common technique to improve partition distribution"},"AWS-Associate-Architect-Notes/Elastic-Load-Balancing,-Amazon-CloudWatch,-and-Auto-Scaling":{"title":"\"Elastic Load Balancing, Amazon CloudWatch, and Auto Scaling\"","links":[],"tags":["AWS","elb","autoscaling"],"content":"These three services can be used very effectively together to create a highly available application with a resilient architecture on AWS. \nElastic Load Balancing\nELB distributes the incoming traffic to multiple EC2 instances in one or more availability zones. ELB supports routing of HTTP, HTTPS, TCP, SSL, DNS, CNAME. They do the health checks of instances so that traffic can’t be transfer to unhealthy EC2, and also launch new instances based on he matrices. \nInternet-Facing Load Balancers \nThey take requests coming from internet and distributes to the EC2 container registered with load balancers. \n\n\nAWS recommends to use DNS rather than IP address of the ELB in order to provide a single, stable entry point. \n\n\nThere are two types of ELBs\nInternal load balancers\nit is often useful to load balance between the tiers of the application.\nHTTPS Load Balancers \nWe can create these load balancers that uses SSL/Transport Layer Security for encrypted connections, for this you have to install certification on your load balancer to decrypt. \nListeners \nListeners are nothing but a process that checks for connection requests. You can allow which kinda connection are allowed and which are not. \nConfiguring Elastic Load Balancing \n\n\nIdle Connection Timeout \n\n\nLoad balancer created two connection when a request comes to the ELB, one is with client and second is with backend services. The idle connection timeout is 60 seconds if there is no data data flow. And if request doesn’t complete within the idle timeout period, the load balancer closes the connection, even if data is still being transferred. \n\n\nKeep-alive, when enabled, allows the load balancer to reuse connections to your back-end instance, which reduces CPU utilisation. \n\n\nCross-Zone Load Balancing \n\n\ntraffic is routed evenly across all back-end instances for your load balancer, regardless of the Availability Zone in which they are located \n\n\nCross-zone load balancing reduces the need to maintain equivalent numbers of back-end instances in each Availability Zone and improves your application’s ability to handle the loss of one or more back-end instances \n\n\nConnection Draining \n\n\nLB stops sending the new connection to the instances that are deregistering or unhealthy, while keeping the existing connections open \n\n\nThe maximum timeout value can be set between 1 and 3,600 seconds (the default is 300 seconds). When the maximum time limit is reached, the load balancer forcibly closes connections to the deregistering instance. \n\n\nProxy Protocol\n\n\nWhen you use TCP or SSL for both front-end and back-end connections, your load balancer forwards requests to the back-end instances without modifying the request headers \n\n\nIf you enable proxy protocol a human readable header is attached with the connection information \n\n\nThe header is then sent to the back-end instance as part of the request \n\n\nIf Proxy Protocol is enabled on both the proxy server and the load balancer, the load balancer adds another header to the request, which already has a header from the proxy server. Depending on how your back-end instance is configured, this duplication might result in errors. \n\n\nSticky Session \n\n\nLad balancer routes each request independently to the registered instance with the smallest load. However, sticky session feature (also known as session affinity),  enables load balancer to bind a user’s session to a specific instance. \n\n\nHealth Checks\n\n\nELB checks all the EC2 which are registered with it, by sending a ping, a connection attempt. If it not responds it labels as OutOfSerivce and ELB does not send the new request to that EC2.  \n\n\nAmazon CloudWatch\nAmazon CloudWatch supports monitoring and specific metrics for most AWS Cloud services, including: \nAuto Scaling, Amazon CloudFront, Amazon CloudSearch, Amazon DynamoDB, Amazon EC2, Amazon EC2 Container Service (Amazon ECS), Amazon ElastiCache, Amazon Elastic Block Store (Amazon EBS), Elastic Load Balancing, Amazon Elastic MapReduce (Amazon EMR), Amazon Elasticsearch Service, Amazon Kinesis Streams, Amazon Kinesis Firehose, AWS Lambda, Amazon Machine Learning, AWS OpsWorks, Amazon Redshift, Amazon Relational Database Service (Amazon RDS), Amazon Route 53, Amazon SNS, Amazon Simple Queue Service (Amazon SQS), Amazon S3, AWS Simple Workflow Service (Amazon SWF), AWS Storage Gateway, AWS WAF, and Amazon WorkSpaces. \n\n\nCloudWatch does not aggregate data across regions but can aggregate across Availability Zones within a region. \n\n\nCloudWatch stores for 2 week, to retain more than that save the logs to S3.\n\n\nAuto Scaling\nAuto Scaling is a service that allows you to scale your Amazon EC2 capacity automatically by scaling out and scaling in according to criteria that you define. \n\n\nManual Scaling \n\n\nWhen you need a very basic, pre-calculated change in your need.\n\n\nScheduling Scaling \n\n\nYou can Schedule when the demand will be at peak and add some compute instances and after that remove those instances can save a lot of bill \n\n\nDynamic Scaling \n\n\nDynamic scaling lets you define parameters that control the Auto Scaling process in a scaling policy\nAuto Scaling Components \nLaunch Configuration: \n\n\nA launch configuration is the template that Auto Scaling uses to create new instances, and it is composed of the configuration name, Amazon Machine Image (AMI), Amazon EC2 instance type, security group, and instance key pair. Each Auto Scaling group can have only one launch configuration at a time \n\n\ndefault number of Amazon EC2 instances you can currently launch within a region, which is 20 \n\n\nAutoScale Group \nEC2 instances are attached to Auto Scale group which are used for scaling purpose. Auto Scaling Group has a Launch configuration file that is used as a base image for deploying new EC2 instances. They have minimum and maximum limit , they also can use spot on! Instances that are way more cheaper than on demand. \nScaling policy \n\n\nYou can associate Amazon CloudWatch alarms and scaling policies with an Auto Scaling group to adjust Auto Scaling dynamically. \n\n\nWhen a threshold is crossed, Amazon CloudWatch sends alarms to trigger changes (scaling in or out) to the number of Amazon EC2 instances currently receiving traffic behind a load balancer. \n\n\nAfter the Amazon CloudWatch alarm sends a message to the Auto Scaling group, Auto Scaling executes the associated policy to scale your group. The policy is a set of instructions that tells Auto Scaling whether to scale out, launching new Amazon EC2 instances referenced in the associated launch configuration, or to scale in and terminate instances. \n\n\nBest practice: \n\n\nScale out fast, but scale in slowly. Because that will avoid the relaunching of new instance as EC2 charges on per hour basis. \n\n\nBootstrap your EC2, that will make ec2 to join faster to the pool. \n\n\nWhen we are doing the AutoScale, we copy the code of frontend and backend to all the machines."},"AWS-Associate-Architect-Notes/Index":{"title":"Index","links":[],"tags":[],"content":""},"AWS-Associate-Architect-Notes/S3-and-Glacier":{"title":"\"S3 and Glacier\"","links":[],"tags":["AWS","security","s3","Glacier","storage","cost"],"content":"Buckets : Global namespace [best practice to save name as organisation name]. \n\n\nName with 63 lowercase number hyphens and periods. \n\n\n100 buckets per account.\n\n\nCreated at a specific reason.\n\n\nBucket can store unlimited number of objects \n\n\nPrivate by default\n\n\nObjects: Entities or files n Amazon in buckets. \n\n\nVirtually can store any data in any format. \n\n\nStorage from 0 kb to 5 TB. \n\n\nKeys: Each s3 Bucket has a unique Identifier called as a key. \nS3 Operations : \n\n\nCreate/Delete buckets. \n\n\nWrite/Read/Delete an object. \n\n\nKey list in buckets \n\n\nDurability and Availability :\n\n\n99.999999999% durability [Will my data still be there?]\n\n\n99.99% availability  [can I access my data right now]?\n\n\nTo protect from accident deletions use versioning, cross-region and MFA Deletion. \n\n\nData Consistency :\n\n\nProvides read-after-write consistency.\n\n\nUpdates to a single key are atomic — you’ll never get a mix of the data (old and new)\n\n\nAccess Control :\n\n\nPrivate by default\n\n\nACLs (Access control List)\n\n\nGrants coarse-grained permissions : READ, WRITE, and Full-Control at bucket level or object level. \n\n\nS3 Bucket policies are recommended access control for S3.\n\n\nBucket policies \n\n\nBucket policies are similar to IAM role but on bucket level. \n\n\nThey include an explicit reference with IAM principle in policies. \n\n\nS3 bucket policies allows cross-accounts access to S3. \n\n\nYou can allow and block anyone using CIDR and during what time of the day. \n\n\nS3 Advance Features \nPrefixes and Delimiters :\n\n\nS3 uses a flat structure in bucket. \n\n\nUse prefix and delimiter for logical organise new data and maintain the hierarchy of the data.\n\n\nCan apply access control on prefixes and delimiters \n\n\nStorage Class. \n\n\nS3 Standard \n\n\nDelivers first-low byte latency and high throughput. \n\n\nBest choice for frequently accessed data. \n\n\nS3 Standard IA(Infrequent Access)\n\n\nSame durability as Standard , low latency, high throughputs. \n\n\nDesigned for long-lived data, less frequent access\n\n\nPricing is less than standard, minimum duration limit of data is 30 days \n\n\nMinimum object with 128kb. \n\n\nper-Gb retrieval cost. \n\n\nRSS (Reduced Redundancy Storage)\n\n\nLower durability 99.99%\n\n\nBetter for thumbnails like easy reproducible data. \n\n\nCheaper than over storages. \n\n\nAmazon Glacier [small intro]\n\n\nBest for storing achieves\n\n\nFree data access to 5% data in glacier. \n\n\nTakes hours to restore \n\n\nMakes a copy to RSS but data retains in the glacier until you delete it. \n\n\nIts’s a standalone service. \n\n\nObject lifecycle Management\nHot(Frequently access data)→ Warm (Less frequent data)→ Cold (Long-term archival data)\nReduce storage cost and we can automate storage class based on the time. \n\n\nInitially on Amazon S3 Standard \n\n\nAfter 30 days, transition to IA \n\n\nAfter 90 days, Glacier \n\n\nAfter 3 years, delete. \n\n\nEncryption \n\n\nBest practice is to Encrypt data at flight and at rest. \n\n\nAmazon SSL Api endpoints \n\n\nAssures that all data will be encrypted while in transit using HTTPS protocol. \n\n\nData encrypt at rest \n\n\nUse SSE (Server Side Encryption)\n\n\nAWS encrypt data at object level before writing to disk and decrypt when you retrieve it. \n\n\nSSL and Amazon KMS (Key management service) uses 256-bit AES (Advance Encryption Standards)\n\n\nClient side Encryption. \n\n\nSSE-S3 (AWS manages key)\n\n\nFully integrated “check-box-style”\n\n\nAWS handles this \n\n\nSSE - KMS (key management Service)\n\n\nKeys are manages by us \n\n\nIncrease access control at object level \n\n\nTrack user activity at object level and user level \n\n\nAttempt to login, failed login. Etc\n\n\nSSE-C (Customer - provided key)\n\n\nYou control every thing \n\n\nBest practice \n\n\nUse server-side encryption with SSE-S3 or SSE-KMS\n\n\nVersioning \n\n\nIt’s a protection against accidental deletion. \n\n\nCreate multiple versions of each object in your bucket. \n\n\nApply at bucket level \n\n\nOnce started it can’t be stopped, only suspended\n\n\nData can be retrieved using previous version of your bucket\n\n\nMFA Delete \n\n\nAdds another layer of data protection at buckets \n\n\nMaybe OTP or hardware for deletion something. \n\n\nMultipart Upload \n\n\nUse for uploading larger files in multi parts\n\n\nBetter network utilisation \n\n\nAbility to pause and resume\n\n\nCan upload with unknown file size\n\n\nIt’s a three way process \n\n\nInitialisation \n\n\nUploading into parts\n\n\nCompletions \n\n\nUpload in any manner \n\n\nAws re arrange the parts at their end \n\n\nIts better to use multi part if file is more than 100 MB \n\n\nCross Region Replication \n\n\nAsynchronously copy S3 bucket from one region to another \n\n\nReduce latency  \n\n\nVersioning must be turned on to both destination and source buckets \n\n\nMust use IAM policy to replicate object on your behalf \n\n\nAny changes on source bucket trigger replication on destination bucket. \n\n\nLogging \n\n\nFor sureliance \n\n\nBest practice is to save log in same bucket using a prefix \n\n\nEvent Notification \n\n\nSetup at bucket Level \n\n\nUses aws SNS or SQS\n\n\nTrigger messages on events on your bucket. \n\n\nBest Practices, patters and performance\n\n\nMake s3 as blob storage and index data to database this enables quick searches and complex queries to run. \n\n\nIf S3 uses extensive GET- requests use Amazon Cloud front distribution as caching layer \n\n\nAmazon Glacier \n\nUsed for long term Backup \nStores in Zip files\n\nArchives \n\nimmutable after created\nData stores in archives which can store upto 40 TB\nCan have unlimited number of archives \nAutomated encrypted \n\nVaults \n\nContainer for archives \nAWS account have limits upto 1000 vaults \nEasily deploy and enforce compliance controls for Glacier vault \nWrite once read many (because they are immutable)\n"},"AWS-Associate-Architect-Notes/SQS,-SNS,-and-SWF":{"title":"\"SQS, SNS, and SWF\"","links":[],"tags":["AWS","SQS","transactions","SWF","workflows","dataProcessing","queue","fanout"],"content":"Amazon Simple Queue Service (Amazon SQS) \nAmazon SQS is a fast, reliable, scalable, and fully managed message queuing service. Amazon SQS makes it simple and cost effective to decouple the components of a cloud application. You can use Amazon SQS to transmit any volume of data, at any level of throughput, without losing messages or requiring other services to be continuously available. \nIf your processing servers cannot process the work fast enough (perhaps due to a spike in traffic), the work is queued so that the processing servers can get to it when they are ready \nDelayed Queues and Timeouts\nDelay queues allow you to postpone the delivery of new messages in a queue for a specific number of seconds \nWhen a message is in the queue but is neither delayed nor in a visibility timeout, it is considered to be “in flight.” You can have up to 120,000 messages in flight at any given time. Amazon SQS supports up to 12 hours’ maximum visibility timeout. \nQueue and Message Identifiers \nAmazon SQS uses three identifiers that you need to be familiar with: queue URLs, message IDs, and receipt handles. \nWhen creating a new queue, you must provide a queue name that is unique within the scope of all of your queues. Amazon SQS assigns each queue an identifier called a queue URL, which includes the queue name and other components that Amazon SQS determines. Whenever you want to perform an action on a queue, you must provide its queue URL. \nDead Letter Queues \nAmazon SQS provides support for dead letter queues. A dead letter queue is a queue that other (source) queues can target to send messages that for some reason could not be successfully processed. A primary benefit of using a dead letter queue is the ability to sideline and isolate the unsuccessfully processed messages. You can then analyze any messages sent to the dead letter queue to try to determine the cause of failure. \nAmazon Simple Workflow Service (Amazon SWF) \nIn Amazon SWF, a task represents a logical unit of work that is performed by a component of your application. Coordinating tasks across the application involves managing inter-task dependencies, scheduling, and concurrency in accordance with the logical flow of the application. \nWith Amazon SWF, you can implement, deploy, scale, and modify these application components independently. \nWorkflows\nSame concept as airflow \nUsing Amazon SWF, you can implement distributed, asynchronous applications as workflows. Workflows coordinate and manage the execution of activities that can be run asynchronously across multiple computing devices and that can feature both sequential and parallel processing. \nWorkflow Domains\nDomains provide a way of scoping Amazon SWF resources within your AWS account. You must specify a domain for all the components of a workflow, such as the workflow type and activity types. It is possible to have more than one workflow in a domain; however, workflows in different domains cannot interact with one another. \nExams Essentials \nKnow how to use Amazon SQS. Amazon SQS is a unique service designed by Amazon to help you to decouple your infrastructure. Using Amazon SQS, you can store messages on reliable and scalable infrastructure as they travel between your servers. This allows you to move data between distributed components of your applications that perform different tasks without losing messages or requiring each component always to be available.\nUnderstand Amazon SQS visibility timeouts. Visibility timeout is a period of time during which Amazon SQS prevents other components from receiving and processing a message because another component is already processing it. By default, the message visibility timeout is set to 30 seconds, and the maximum that it can be is 12 hours. \nKnow how to use Amazon SQS long polling. Long polling allows your Amazon SQS client to poll an Amazon SQS queue. If nothing is there, ReceiveMessage waits between 1 and 20 seconds. If a message arrives in that time, it is returned to the caller as soon as possible. If a message does not arrive in that time, you need to execute the ReceiveMessage function again. This helps you avoid polling in tight loops and prevents you from burning through CPU cycles, keeping costs low. \nAmazon SWF gives you full control over implementing tasks and coordinating them without worrying about underlying complexities such as tracking their progress and maintaining their state.\nAmazon SNS is a push notification service that lets you send individual or multiple messages to large numbers of recipients. Amazon SNS consists of two types of clients: publishers and subscribers (sometimes known as producers and consumers). Publishers communicate to subscribers asynchronously by sending a message to a topic."},"AWS-Associate-Architect-Notes/Security-and-IAM":{"title":"\"Security and IAM\"","links":[],"tags":["AWS","security","iamRoles"],"content":"AWS Identity and Access Management (IAM) \n\nusers (people or services that are using AWS)\ngroups (containers for sets of users and their permissions)\nroles (containers for permissions assigned to AWS service instances)\n\nIAM uses traditional identity concepts such as users, groups, and access control policies to control who can use your AWS account, what services and resources they can use, and how they can use them. \n\n\nIAM is not an identity store/authorisation system for your applications \n\n\nPrincipals\nPrincipal is an IAM entity that is allowed to interact with AWS resources. \nThere are three types of principals :\n\n\nroot users\n\n\nWhen you first create an AWS account, you begin with only a single sign-in principal that has complete access to all AWS Cloud services and resources in the account \n\n\nIAM users\n\n\nUsers are persistent identities set up through the IAM service to represent individual people or applications. \n\n\nUsers are persistent in that there is no expiration period; they are permanent entities that exist until an IAM administrator takes an action to delete them. \n\n\nroles/temporary security tokens \n\n\nThese tokens are issued for a small period of time. When one of the actors assumes a role, AWS provides the actor with a temporary security token from the AWS Security Token Service (STS) \n\n\nThe range of a temporary security token lifetime is 15 minutes to 36 hours. \n\n\nEC2 roles \n\n\nAssign IAM role to access Ec2 and S3 storage. \n\n\nCross-region Access\n\n\nThis is useful when you have \n\n\nAuthorization \nThe process of specifying exactly what actions a principal can and cannot perform is called authorization \nPolicies\nA policy is a JSON document that fully defines a set of permissions to access and manipulate AWS resources. Policy documents contain one or more permissions, with each permission defining: \n\n\nEffect—A single word: Allow or Deny.\nService—For what service does this permission apply? Most AWS Cloud services \n\n\nSupport granting access through IAM, including IAM itself. \n\n\nResource—The resource value specifies the specific AWS infrastructure for which this permission applies. This is specified as an Amazon Resource Name (ARN). The format for an ARN varies slightly between services, but the basic format is: “arn:aws:service:region:account-id:[resourcetype:]resource”. \n\n\nAssociating Policies with Principals \nA policy can be associated directly with an IAM user in one of two ways: \nUser Policy: These policies exist only in the context of the user to which they are attached \nManaged Policies: These policies are created in the policies tab on IAM page, these are independent and can be associated with any group or any single user. You can also write your own policy specific to your case. \nGroup policy:  These policies exist only in the context of the group to which they are attached. \nBest practice is to create a new IAM group called “IAM Administrators” and assign the managed policy, “IAMFullAccess.” Then create a new IAM user called “Administrator,” assign a password, and add it to the IAM Administrators group.\nMFA(Multi-Factor Authentication)\nWith MFA, authentication also requires entering a One-Time Password (OTP) from a small device. The MFA device can be either a small hardware device you carry with you or a virtual device via an app on your smart phone (for example, the AWS Virtual MFA app). \n\nIn multiple permission if there is any not any explicit allow, default is deny.\n"},"AWS-Associate-Architect-Notes/VPC-(Virtual-Private-Cloud)":{"title":"\"VPC (Virtual Private Cloud)\"","links":[],"tags":["AWS","VPC","networking"],"content":"VPC (Virtual Private Cloud)\nIt’s a custom defined virtual network on aws cloud. It’s a networking layer for EC2\n\n\nVPC is Logically Isolated even if It shares the IP address \n\n\nCan span along the availability zones \n\n\nWhen creating VPC we must select a CIDR range \n\n\nIPV4 address\n\n\nRange of CIDR can’t be changed after creation \n\n\nRange can be between /16 (65,536 available address) or /28 (16 available address)\n\n\nVPC consist of Following Component \n\n\nSubnets \n\n\nRoute tables\n\n\nDHCP (Dynamic Host Config Protocol) option sets \n\n\nSecurity Groups \n\n\nNetwork Security Control List (ACL)\n\n\nOption components \n\n\nInternet Gateways \n\n\nElastic IP \n\n\nElastic Network Interfaces \n\n\nEndPoints \n\n\nPeering \n\n\nNAT instances and Gateways \n\n\nVPG (gateway) \n\n\nVPN\n\n\nCustomer gateway \n\n\nSubnets \n\n\nSegment of VPC IP address range where we can launch aws services \n\n\nCIDR block defines subnet (192.0.1.0/24) or (192.0.0.0/24)\n\n\nAWS reserves first 4 address and last IP for itself. If you have a /28 or 16 IP addresses, AWS will save 5 for itself and you will end up having 11 addresses \n\n\nYou can add one to more subnets in one availability zone , therefore multiple subnets in one availability zone.\n\n\nSubnets don’t span across the availability zone for connecting availability zone you have to use the route table or a NAT \n\n\nClassifier into 3 classes \n\n\nPublic \n\n\nThis is used for routing internet to the VPC’s Internet Gateway\n\n\nAssociated with route table \n\n\nPrivate \n\n\nThis is used internally in VPC, does not direct traffic to IGW of VPC \n\n\nAssociated with route table\n\n\nVPN-only \n\n\nAws VPC contains one public subnet in every AZ within the region with net-mask of /20. \n\n\nRoute Tables\n\n\nRoute tables creates a logical connection(construct) in VPC. Tell’s how traffic will direct in VPC \n\n\nVPC comes with a Default route table called local route which enables communication in VPC and this can’t be modified or deleted. \n\n\nWe can specify which subnets will become public and private by directing traffic through the IGW or not respectively \n\n\nUsing route tables EC2 in different subnets can communicate to each other. \n\n\nWe can add custom routes. \n\n\nPoints to remember \n\n\nVPC has implicit router \n\n\nVPC comes with main route table \n\n\nWe can create custom route tables \n\n\nEach subnet must be connected with a route table other wise it will use the main route table \n\n\nYou can replace default with custom route table so that all custom route table automatically associates with it \n\n\nEach route in table specifies a destination CIDR and a target\n\n\nInternet Gateways \n\n\nIt’s a VPC component with horizontally scaling, redundant, high availability. \n\n\nAllows communication between instances and the internet using public IP addresses \n\n\nHow to create a public IP addresses \n\n\nAttach IGW to VPC \n\n\nCreate sub route table rule to send all non-local traffic  IGW \n\n\nConfigure ACL to allow traffic to flow to and from the instance \n\n\nAssign EIP or public IP to the Instance \n\n\nDHCP \nWill do this later\nElastic IP \n\n\nAws has a pool of public IP addresses in each region \n\n\nEIP is a static IP address\n\n\nWe can take if from the pool and release it when the work is done \n\n\nEIP remains fixed and allows you to change the underlying infrastructure \n\n\nPoints to remember \n\n\nFirst allocate the EIP for VPC then attach to the Instance \n\n\nEIP is specific to a region\n\n\nOne-to-one relationship between network Interfaces and EIP\n\n\nCan move from one instance to another to same or in another VPC  but in same region \n\n\nEIP is associated with your account until you release it [important]\n\n\nEIP is chargeable either you use it or not \n\n\nElastic Network Interface (still confused )\n\n\nHelps you to create a management network, use network and appliance in your VPC \n\n\nCreate a dual-homed instance with workload/roles on distinct subnets \n\n\nCreate a low-budget , high availability solution \n\n\nEndpoints \n\n\nVPC endpoints enables you to create a private connection layer between AWS VPC and other services without using internet or Nat, direct connect. \n\n\nWe can create multiple endpoints for subnets for accessing same service\n\n\nPoint to remember for VPC endpoints \n\n\nSpecify amazon VPC \n\n\nSpecify the service \n\n\nSpecify the policy \n\n\nSpecify the route tables \n\n\nIf the service is in other region then it will use the IGW not the specified endpoint because it only works for same region. \n\n\nPeering \n\n\nNetworking Connection between two or more VPCs and act like they are in within same network \n\n\nIt can only work if both VPC are in same region \n\n\nYou can also build a networking connection to other account’s VPC \n\n\nIts a one-to-one relationship \n\n\nIt is created through a request/acept protocol, \n\n\nDo not support transitive routing \n\n\nPoints to remember \n\n\nCan’t create peering if VPC is in different region \n\n\nCan’t have more than one peering connection between the same two Amazon VPC at the same time. \n\n\nCan’l create peering if two VPC have overlapping or matching CIDR blocks \n\n\nSecurity Groups \n\n\nIt’s a virtual stateful firewall for AWS controls inbound and outbound, if response is allowed inbound rule to will also allow outbound rule regardless of outbound rule  \n\n\nImportant rules to remember \n\n\nBy default no inbound traffic is allowed \n\n\nYou can specify allow rules not deny rules (This is the difference between ACL and security group) \n\n\nBy default all outbound rules are allowed \n\n\nInstance of same security group can’t talk until you add rules \n\n\nChanges in Security groups are reflected immediately \n\n\nNetwork Allocated Control List \n\n\nIts also a firewall but works on subnet level and stateless\n\n\nBy default all inbound and outbound rules are deny \n\n\nVPC are created with modifiable ACLS \n\n\nEvery subnet must be associated with ACL \n\n\nDifference between ACL and Security groups \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecurity GroupACLInstance levelSubnet levelAllows rule onlyAllow and deny rulesStatefull : Returned traffic is allowedStateless: üst be explicitly allowed y rulesAWS evaluates all rules before deciding whether to allow trafficAWS processes rules in number whether to allow trafficOn individual instancesAutomatically apply to all instance associated with ACL\nNAT and NAT Gateways \nIn AWS VPC by default  instance in private IP  can’t connect with the internet through IGW. For this AWS has a NATInstance, AWS recommend to use NAT Gateway rather than NAT Instance because of high availability and higher bandwidth. \nNat Instances (Network address Translator)\n\n\nPoints otIts an linux AMI designed to accept traffic from private IP and translate to public IP for internet access \n\n\nFor Nat do the following \n\n\nCreate security group   \n\n\nVPG, VPNs, CGWs\n\n\nYou can attach your data centre to a AWS VPN.\n\n\nAWS VPC offers two ways to connect to a corporate network to a VPC: VPG and CWG \n\n\nVPG (Virtaul provate gateways)\n\n\nis a VPNs connector between two networks on AWS side\n\n\nCGW\n\n\nIs a customer side VPN connector \n\n\nThen there is a VPN tunnel where the data will flow between AWS and customer datacenter \n\n\nVPN \n\n\nIs generated after traffic flows from customer side \n\n\nyou must specify the type of connection you’ll have in tunnel. \n\n\nIf customer supports border gateway then configure routing for dynamic routing \n\n\nIf Static routing then you must enter the for your network that should be comminucated of VPG. \n\n\nVPC supports multiple CWG (Many to one)\n\n\nEach CWG have his own tunnel \n\n\nPoints to remember \n\n\nVPG  is a endpoint of the tunnel of aws \n\n\nYou must create connection from CWG to VPG \n\n\nVPN consist of two tunnels or higher availability\n\n"},"FinanceKernal/Data-Collection-And-Database-Choice":{"title":"\"Data Collection And Database Choice\"","links":["Tech/Efficient-Data-Processing-on-a-Budget---My-Journey-with-File-Based-Queues","Tech/A-Personal-Tale-of-Data-Transformation---The-Simple-Genius-of-jq,-GNU-Parallel,-and-a-Task-Queue"],"tags":["Algorithm","AlgoTrading","architecture","Async","mongodb","queue"],"content":"Scrapping Process\n\n  Write a process in golang for downloading data from bse to download history  Efficient Data Processing on a Budget - My Journey with File-Based Queues\n Write a process on golang for downloading intraday data A Personal Tale of Data Transformation - The Simple Genius of jq, GNU Parallel, and a Task Queue\n\nDatabase Selection\nFirst choice for MYSQL second was influxDB and third was mongodb. I went with mongodb as I did not knew what I will store and second I was eaiser to setup on ec2 with docker.\nMaybe later I can migrate to somthing better maybe a files based storage. (parquet, avro stuff) and put them in s3, and use minIO for fetching the data."},"FinanceKernal/High-Level-Design":{"title":"\"High Level Design\"","links":[],"tags":["financekernel","basicIdea","AlgoTrading"],"content":"FinanceKernel: High level Design \nQ: We Have NSE and BSE, over 4000+ listed stocks, how to choose which stocks to select for the portfolio. \nA: We have 3 Types, Large, mid, small. \nWe might need to create 3 portfolio and divide investment amongst them.\nFundamental Analysis (Manual process or maybe a script for this)\nThese are stable companies as they are big and dont take much risk. We can filter them based on Fundamental ratios like P/E, P/B, Intrinic value and other ratios. Make find some rules for swing trading for fundamential ratios. \nTechincal Indicators and charts. \nWe need to understand the rules of tedhcnical indicators how they work on time lags. This will tell the history and tendency. \nWe also need to understand about charts gives us entry and exist points hence give us the rules of taking the bet if it will go down or up on that day or week etc when combined with Technials.\nThis will give us the list of stock to trade or invest into.  \nQ: We might have 1000 Stocks after this. How to pick now ? \nA: We need to can pic top 50 stock for each portfolio.\nQ: How do you validate this process that selection is good?\nA: We can implement a backtest on all those companies and check if we are profitable based on our timelines and target. We maybe just can use some basic rules for sell and buy and check the profitability. And compare this with Nifty 50 performance.  \nThis will give a +ve signal if the filtering process was right or not.\nQ: As we don’t know what impacts the price to move up or down. How to learn this without learning as might have very naive idea like moving average charting etc. \nA: Based on our knowledge of how technical analysis works we might be able to model that ? Plus we can add some time series feature extraction that can identify  the pattern. \nQ: how to Model this problem into ML ? Or DL or reinforcement Learning ? \nA: We can see this as multi process step or more of a pipeline. \nFirst target is to model the market or learn what parameters have to be set in order to make the profit. \nAs a feature we can add the technical indicators as they tells a lot, we can add charting data as well that\nThe Target need to labeled, we can label based on multiple factors like, mark the data true if the profit of the stock next day is &gt; 1% and last 3 days were in profit as well. Or maybe we just focus on a model that can tell me when will stock go down. If it went up yesterday. We can learn these patterns. \nWe can just use the idea of triple barrier method as well, set out profit and loss criteria, like 1 % is good but loss of 1 % is not good. We have to fix the timeline first. Let’s just focus on 1 day, \nWe aim to learn the patterns using technical data and charting that when was I able to make profit &gt; 1 % and when did I make a loss. \nModel will tell us to buy a stock or not. It will not tell how much this will increase but it will increase by 1% with some confidence levels. Same model should tell me how much a stock will go down as well. \nWith these signals, we will trust our system and run it through out backtesting pipeline, if it performs better than our stock selection process that means model labeling works and feature selection also works. \nQ: How to manage for loss and diversification. ? \nWe will not go into that as of now. Let’s work on baby steps."},"FinanceKernal/Index":{"title":"Index","links":[],"tags":[],"content":""},"FinanceKernal/Introduction":{"title":"\"Introduction\"","links":[],"tags":["Finance","Algorithm","AlgoTrading","SillyProject"],"content":"Project Overview: Building a System for Consistent Daily Profits\nThe goal of this project is to explore the feasibility of building a system that can consistently achieve an average daily profit of 1%. Although it is widely recognized that this task is practically impossible due to the generally efficient nature of financial markets, the project serves as a valuable learning experience. Key areas of focus include:\n\n\nUnderstanding Time Series Data:\n\nLearning how to handle and analyze time series data, which is critical for financial modeling and forecasting.\n\n\n\nData Handling and Processing:\n\nExploring techniques for managing large datasets, including data cleaning, transformation, and normalization.\n\n\n\nDatabase Selection:\n\nInvestigating different databases to determine which are best suited for storing and querying financial data efficiently.\n\n\n\nPortfolio Selection:\n\nLearning how to select a diversified portfolio of assets to optimize returns and manage risk.\n\n\n\nRisk Management:\n\nImplementing strategies to minimize losses and protect capital, including stop-loss orders, position sizing, and diversification.\n\n\n\nWhile the primary objective may be ambitious, the knowledge and skills gained through this project will be invaluable for understanding the complexities of financial markets and quantitative trading."},"FinanceKernal/Low-Level-System-Design":{"title":"\"Low Level System Design\"","links":[],"tags":["AlgoTrading","financekernel","basicIdea"],"content":"FinanceKernel: Low Level Design\nQ: How to setup the flow ? which thing should we build first ?\nA: We need to learn about fundamental stuff and which technical indicators works for eveyone in swing trading. \nAlso how charting works and which charts to include. \nBuild an understanding of it and the create a backtesting unit test for it. Create the backtesting module using OOPS logics this will help a lot and make it configurable for strategies. \nTry to make profit by the understanding of techincals and charting. code it and test it. \nQ: Stock Selector Module. \nBuild this next time, this will filter down to 200 - 500 potential stocks. The use ML to filter stock that will give profit in next day using ML pipeline. \nQ: Signal Module. \nThis model should tell, should we but that filter stock or not, what should be the size of our trade and the risk analysis as well. Even if stock looks promising and its ambguos to buy then we dont do trade it. \nQ: Trader Module.\nThis will be done manually."},"FinanceKernal/Trading-System-Requirements":{"title":"\"Trading System Requirements\"","links":[],"tags":[],"content":"FinanceKernel: Trading system requirements. \n\n\nBuild a system based on swing trading using software principles and ML software engineering. \n\n\nWhat is swing trading and what is necessary for build the software ?\nIt’s a small to mid term investment strategy majorly depends upon mean either trend or mean reversal. \nPrerequisite for the software. .?\nInitial amount: 10000. \nHow much money can I loose in a day or in a specific time period: at any given day my any of my selected stock should not fall less than 3% and whole portfolio shouldn’t fall less than 2%. \nWhat is success to you: if all the stocks are increased by 1% then and portfolio by 2 that makes sense. On a daily basis. \nHow many portfolios do we need ? \n\nFor faster trading 1 day or 2 day. \nFor long term trading like 7 days for 15 days. \n\nEach portfolios will have 10k in them. \nExpected output from the software ? \nThe expected output is the list of stock for each portfolio in a sorted order of profitability. That is refreshed every day. \nHow do we measure the performance of the system and how will feedback work ?\nShort term portfolio will be measured based on the end of the day performance. 2 ways to look at the matrices. \n\n\nMy overall investment should not go down more than 1 % on that day. Is good \n\n\nIf increase of portfolio is 1% or greater than that is consider good. \n\n\nIf 80% of the company have move 1% is good. \n\n\nCollect these matrices for 10 days and check the performance. If system is able to give these result 80% is considered good. That mean 8 days were profitable but 2 days were not profitable.\nHow will feedback will work ? \nWe need this is as a reinforcement learning like system. This is not well thought yet how it will work. TBD."},"Low-Level-Desing/Index":{"title":"Index","links":[],"tags":[],"content":""},"Low-Level-Desing/Interviers---LeetCode":{"title":"\"Interviers - LeetCode\"","links":[],"tags":[],"content":""},"Low-Level-Desing/Interviews---Hotel-Booking-(MMT)":{"title":"\"Interviews - Hotel Booking (MMt)\"","links":[],"tags":[],"content":""},"Low-Level-Desing/Interviews---ParkingLot-low-Level-Design":{"title":"\"Parking Lot low Level Design\"","links":[],"tags":["LLD","ParkingLot","OOPS","DesignPrinciples","component"],"content":"Parking Lot Design: Navigating the Tech Interview Maze\nI’ve been diving into the world of tech interviews lately, and I’ve got to admit, it’s been a bit disappointing. It seems like companies are all following the same interview trends, and “content creators” are cashing in on this pattern. I decided to check out Agoda’s interview experiences on LeetCode, and they stood out with their well-structured approach.\nWhat really caught my attention were the Low-Level Designs (LLDs). I’ve never really delved into them from an interview perspective before. While exploring, I stumbled upon a bunch of videos on how to tackle seemingly simple problems like designing a parking lot. Some folks nailed it with a smart and sensible approach, while others seemed to throw every coding pattern they knew at the problem.\nThe issue here is that newcomers often end up memorizing solutions to specific problems without really understanding the core concepts. Striking a balance between problem-solving finesse and grasping the basics is key.\nIn interviews, companies may require diagrams or code. We’ll explore creating both without overusing design patterns and emphasize crucial interview points.\nRequirement Gathering. \nOne of the major points in both LLD and HLD phases. If you can master this art, you can navigate a lot of projects in your company as well. \n“Imagine you’re tasked with designing a parking lot management system. How would you gather requirements for this project?”\nI’ll avoid sticking to the typical interview pattern and help us grasp the essence of it. This phase is crucial from a developer’s viewpoint as it demonstrates the ability to think beyond coding—a key quality sought after by top companies in their developers.\n\n\nFunctional Requirements: Define how vehicles enter, exit, reserve spots, pay fees, and track occupancy.\n\n\nInput and Output: Capture vehicle information upon entry, issue parking tickets, accept payments, and provide receipts upon exit.\n\n\nEdge Cases and Error Handling: Handle scenarios like a full lot, lost tickets, and system failures gracefully.\n\n\nPerformance Considerations: Optimize for high-volume usage during peak hours, ensuring efficient entry and exit processes.\n\n\nSecurity: Implement user authentication, data protection, and surveillance to safeguard the parking lot.\n\n\nData Management: Store and manage vehicle data, reservations, and payments securely with data validation and backups.\n\n\nUsability: Design user-friendly interfaces, clear signage, and conduct usability testing for a seamless experience.\n\n\nTesting: Develop and execute tests, including unit tests and load testing, to ensure reliability and performance.\n\n\nMaintenance: Plan for ongoing maintenance, bug fixes, security updates, and potential future enhancements.\n\n\nThese points represent the foundation of software development. Interviews may emphasize specific aspects, but understanding the entire development pipeline is crucial. While there are many books on requirement gathering, let’s focus on these essentials for now.\nFunctional Requirements\nIn an interview or when engaging with stakeholders, asking and clarifying functional requirements is crucial to understand the project’s scope. Using the example of a parking lot system:\n\n\nAsk Open-Ended Questions: Begin by asking open-ended questions to gather initial insights. For instance, you can ask, “Could you describe how vehicles should interact with the parking lot system?”\n\n\nIdentify Core Functionalities: Dive deeper to identify the core functionalities. Ask questions like, “Should the system allow vehicles to reserve parking spaces in advance?” or “What are the key actions users should perform, such as entry, exit, or payment?”\n\n\nDiscuss User Roles: Clarify who the primary users are and their roles. Ask, “Are there different types of users, like customers, parking attendants, or administrators? What tasks do they need to perform?”\n\n\nCapture Specifics: Request specifics about each functionality. For example, “When a vehicle enters, should it provide its license plate number, and should the system issue a digital or printed parking ticket?”\n\n\nHandle Exceptional Cases: Inquire about exceptional scenarios. Ask, “What happens if the parking lot is full, or if a user loses their parking ticket? How should these situations be handled?”\n\n\nPayment and Fee Calculation: If payment is involved, clarify how it should work. Ask, “What payment methods should be supported, and how should parking fees be calculated – hourly, daily, or otherwise?”\n\n\nReal-Time Monitoring: If relevant, discuss real-time monitoring requirements. Ask, “Should the system provide real-time information on available parking spaces?”\n\n\nBy asking these questions and seeking clarification, you demonstrate your ability to understand and document functional requirements effectively. This approach ensures that you and the stakeholders are on the same page and helps prevent misunderstandings. \nDetermining the domain models\nCertainly! Let’s walk through the process of identifying domain models, determining classes, and defining their methods for a parking lot system:\n1. Understand the Problem Domain:\n   - We want to create a system to manage a parking lot. This includes vehicles entering and exiting, reserving spots, issuing tickets, processing payments, and monitoring occupancy.\n2. Identify Nouns and Verbs:\n   - Nouns: Vehicle, ParkingSpot, Ticket, Payment, User.\n   - Verbs: Enter, Exit, Reserve, CalculateFee, Validate.\n3. Create an Initial List of Classes:\n   - Vehicle\n   - ParkingSpot\n   - Ticket\n   - Payment\n   - User\n4. Define Class Attributes:\n   - Vehicle:\n     - Attributes: licensePlate, entryTime, exitTime\n   - ParkingSpot:\n     - Attributes: spotNumber, isOccupied, vehicle (to store the parked vehicle)\n   - Ticket:\n     - Attributes: ticketNumber, entryTime\n   - Payment:\n     - Attributes: paymentMethod, amount\n   - User:\n     - Attributes: username, password, role\n5. Determine Class Methods:\n   - Vehicle:\n     - Methods: enterParkingLot, exitParkingLot\n   - ParkingSpot:\n     - Methods: occupy, vacate\n   - Ticket:\n     - Methods: generateTicket\n   - Payment:\n     - Methods: processPayment\n   - User:\n     - Methods: login, logout\n6. Establish Relationships:\n   - Vehicle has a relationship with Ticket (one-to-one), as each vehicle corresponds to a ticket.\n   - ParkingSpot has a relationship with Vehicle (one-to-one or one-to-many), indicating which vehicle is parked there.\n   - User may have a relationship with Ticket (one-to-many), as users may have multiple parking tickets.\n7. Refine and Review:\n   - Share your class definitions and relationships with stakeholders or peers to ensure they align with the parking lot system’s requirements.\n8. Consider Inheritance and Interfaces:\n   - You might consider creating a common interface or base class for objects that require payment, like Ticket and Payment, to share payment-related methods.\n9. Document Your Domain Model:\n   - Create a visual representation (e.g., a UML diagram) to document your domain model, including classes, attributes, methods, and relationships.\n10. Iterate and Test:\n   - As you work on the project, be open to making adjustments to your domain model based on evolving requirements or feedback from testing.\nThis step-by-step process helps you systematically define the domain models, classes, attributes, and methods for your parking lot system, ensuring clarity and alignment with the project’s goals."},"Tech/A-Personal-Tale-of-Data-Transformation---The-Simple-Genius-of-jq,-GNU-Parallel,-and-a-Task-Queue":{"title":"A Personal Tale of Data Transformation - The Simple Genius of jq, GNU Parallel, and a Task Queue","links":["tags/collegedays","tags/goodtimes"],"tags":["GNU","parallel","jq","queue","collegedays","goodtimes"],"content":"As a lead backend engineer and an avid finance enthusiast, my journey has blended advanced programming with machine learning insights in the finance sector. Starting with Python, then moving to JavaScript and Go for their respective strengths, I eventually found that their capabilities were stretched by the demanding pace of my tasks. This led me to discover the efficiency of jq, GNU Parallel, and a file-based task queue. This powerful trio not only streamlined my stock market projects but also proved invaluable for parsing application logs and transforming data across various domains. Their impact was so profound that I’ve since incorporated them into complex tasks like transferring millions of events swiftly using SQS. I’m excited to share how these tools have simplified my approach beyond traditional programming.\nWhat is jq?\njq is like a Swiss Army knife for JSON data. It’s a lightweight and flexible command-line JSON processor. You can slice, dice, transform, and reshape JSON data in whichever way you fancy. The best part? Its syntax is intuitive and powerful, perfect for quick one-liners or complex transformations.\nLet’s take an example json that I build myself (chatGPT) for better understanding.\n{  \n  &quot;products&quot;: [  \n    {&quot;id&quot;: 1, &quot;name&quot;: &quot;Laptop&quot;, &quot;price&quot;: 800, &quot;inStock&quot;: true},  \n    {&quot;id&quot;: 2, &quot;name&quot;: &quot;Smartphone&quot;, &quot;price&quot;: 500, &quot;inStock&quot;: false},  \n    {&quot;id&quot;: 3, &quot;name&quot;: &quot;Tablet&quot;, &quot;price&quot;: 300, &quot;inStock&quot;: true},  \n    {&quot;id&quot;: 4, &quot;name&quot;: &quot;Smartwatch&quot;, &quot;price&quot;: 200, &quot;inStock&quot;: true},  \n    {&quot;id&quot;: 5, &quot;name&quot;: &quot;Camera&quot;, &quot;price&quot;: 150, &quot;inStock&quot;: false}  \n  ]  \n}\n\nFiltering by Price and Stock Availability\n\nTask: Select products priced over $300 and currently in stock.\nCommand: jq &#039;.products[] | select(.price &gt; 300 and .inStock == true)&#039; products.json\nOutput: { &quot;id&quot;: 1, &quot;name&quot;: &quot;Laptop&quot;, &quot;price&quot;: 800, &quot;inStock&quot;: true }\n\nCalculating Average Price\n\nTask: Compute the average price of all products.\nCommand: jq &#039;[.products[].price] | add / length&#039; products.json\nOutput: 390\n\nCombining and Deduplicating Data from Multiple Files\n\nAdditional File (additional_products.json): [{&quot;id&quot;: 6, &quot;name&quot;: &quot;Headphones&quot;, &quot;price&quot;: 100, &quot;inStock&quot;: true}, {&quot;id&quot;: 1, &quot;name&quot;: &quot;Laptop&quot;, &quot;price&quot;: 800, &quot;inStock&quot;: true}]\nTask: Merge product data from two files, removing duplicates.\nCommand: jq -s &#039;add | unique_by(.id)&#039; products.json additional_products.json\n\n[\n{“id”: 1, “name”: “Laptop”, “price”: 800, “inStock”: true},\n{“id”: 6, “name”: “Headphones”, “price”: 100, “inStock”: true}\n]\nConverting JSON to CSV Format\n\nCommand: jq -r &#039;.products[] | [.id, .name, .price] | @csv&#039; products.json\n\n1,&quot;Laptop&quot;,800  \n2,&quot;Smartphone&quot;,500  \n3,&quot;Camera&quot;,150\n\nAs you can see we can do the things that we used to to with pandas or javascript etc. Imagine doing this calculation on mongodb exported documents data to maybe on an elastisearch query response (Elasticsearch has REST endpoints unlike others).\nNow comes the GNU Parallel.\nWhat is GNU Parallel?\nGNU Parallel is a tool for executing jobs in parallel, using one or more computers. It’s like a magic wand that takes a bunch of tasks and distributes them across available CPU cores, speeding up processing significantly. It’s particularly useful for repetitive tasks that can be run concurrently.\nTask: Run a complex command involving pipes and redirections.\nCommand: parallel &quot;grep -E &#039;^[^2][0-9]{2} &#039; {} | wc -l &gt; {}.count&quot; ::: access_log*.txt\nUseCase: I heavily use this when searching things in logs, specifically when shit hits the floor # funtimes\nTask: You want to run a set number of jobs in parallel from a larger list of tasks.\ncommand: cat urls.txt | parallel -j 10 curl -O\nUseCase**:** This will download files from a list of URLs in urls.txt, running 10 downloads in parallel at any time. (Torrents I guesscollegedays), I use it for data scrapping POC though # fuckMyLife.\nTask: Execute tasks in parallel but keep the output in the same order as input.\ncommand: parallel — keep-order echo ::: $(seq 1 5)\nUseCase: because sometime you need order in your life.\nTask: Suppose you have a script process.sh that takes two arguments, and you want to run it for different combinations of arguments in parallel.\ncommand: parallel ./process.sh ::: arg1 arg2 arg3 ::: argA argB\nThis command will execute ./process.sh with all combinations of the arguments, resulting in six executions:\n- ./process.sh arg1 argA\n- ./process.sh arg1 argB\n- ./process.sh arg2 argA\n- ./process.sh arg2 argB\n- ./process.sh arg3 argA\n- ./process.sh arg3 argB\n\nUseCase: I used this to test our system with different settings, revealing varying outcomes. We suspected the database might cache data, so we gave it a shot. Surprisingly, we ran ElasticSearch and Spring Boot on a modest 2 GB server, hitting an impressive average of 300+ requests per second on an EC2 instance with 30 concurrent requests. This convinced us to embrace Docker for ElasticsSearch and our apps, giving us more control over hardware and the power to maximize CPU usage.goodtimes.\nHow to Emulate File-Based Queue Logic for Resuming Processing?\nThe brilliance of a file-based task queue lies in its simplicity and dependability. Here’s a quick guide on how to set it up:\n\nCreate a Task File: This file lists all the tasks to be processed, one per line, like a to-do list.\nProcess with GNU Parallel: Run GNU Parallel, directing it to your task file. It picks up each task and executes it.\nLogging Progress: GNU Parallel keeps a progress log. If the process halts, you can resume it, and Parallel will continue from where it stopped, skipping completed tasks.\nEasy Resumption: This setup ensures that interruptions don’t require starting from scratch. Simply run the command, and it continues processing the remaining tasks.\n\nCreate a tasks file\necho &quot;task1\\ntask2\\ntask3&quot; &gt; tasks.txt  \n\nProcess with GNU Parallel\nparallel --joblog progress.log &lt; tasks.txt\n\nAn example of web scraping using a combination of tools like jq, GNU Parallel, and a file-based queue and utilising the DummyJSON website (chatGPT). Keep in mind that you can also use these tools individually for different purposes.\n#!/bin/bash  \n\nCreate a task file with URLs\necho &quot;dummyjson.com/products/1&quot; &gt; tasks.txt  \necho &quot;dummyjson.com/products/2&quot; &gt;&gt; tasks.txt  \necho &quot;dummyjson.com/products/3&quot; &gt;&gt; tasks.txt  \necho &quot;dummyjson.com/products/4&quot; &gt;&gt; tasks.txt  \necho &quot;dummyjson.com/products/5&quot; &gt;&gt; tasks.txt  \n\nCreate a queue file to store failed tasks\ntouch queue.txt  \n\nCreate a CSV file with headers\necho &quot;id,title,description,price,discountPercentage,rating,stock,brand,category&quot; &gt; output.csv  \n\nFunction to process a task and append to CSV\nfunction process_task() {  \n    local url=&quot;$1&quot;  \n      \n    # Download JSON response and convert to CSV  \n    local csv_data  \n    csv_data=$(curl -s &quot;$url&quot; | jq -r &#039;.id, .title, .description, .price, .discountPercentage, .rating, .stock, .brand, .category&#039; | tr &#039;\\n&#039; &#039;,&#039;)  \n  \n    # Check HTTP status code  \n    local http_status  \n    http_status=$(curl -s -o /dev/null -w &quot;%{http_code}&quot; &quot;$url&quot;)  \n  \n    if [ &quot;$http_status&quot; -eq 200 ]; then  \n        # Append CSV data to output.csv  \n        echo &quot;$csv_data&quot; &gt;&gt; output.csv  \n        # Remove the processed task from tasks.txt  \n        grep -v &quot;$url&quot; tasks.txt &gt; tasks_tmp.txt  \n        mv tasks_tmp.txt tasks.txt  \n    else  \n        echo &quot;Failed: $url (HTTP Status: $http_status)&quot;  \n        # Append the failed task to the queue file for retry  \n        echo &quot;$url&quot; &gt;&gt; queue.txt  \n    fi  \n}  \n  \n\nExport the function so it can be used with parallel\nexport -f process_task  \n\nProcess tasks with GNU Parallel\ncat tasks.txt | parallel -j10 process_task  \n\nClean up JSON files\nrm -f *.json  \n\nVerify that tasks.txt and queue.txt are empty\nif [ ! -s tasks.txt ]; then  \n    rm tasks.txt  \nfi  \nif [ ! -s queue.txt ]; then  \n    rm queue.txt  \nfi\necho &quot;Processing completed.&quot;\n\nI’m pondering whether to write about how I turned a stock prediction challenge into a machine learning adventure or perhaps dive into the showdown between Java virtual threads and Spring WebFlux. Well, that’s the plan for now. Until then, jai ram ji ki."},"Tech/Battle---NodeJs-vs-Spring-Webflux":{"title":"Battle - NodeJs vs Spring Webflux","links":[],"tags":["nodejs","java","spring-webflux","benchmark"],"content":"Out of frustration and anger I decided to do compare these two framework\nI don’t have anger issue but after having conversation with developers and reading on blogs etc. It seems everyone favours Node.js capabilities when it come to developing I/O applications and its irritating to hear that Java is not a good choice for it.\nHere is what I have found from stackoverflow about what is I/O task and CPU intensive tasks are as the meaning of I/O heavily depends on prespective. I’m still not sure if these definition are correct. I’m open to disscuss and change them.\nPlease do tell if the code is not optimised or if I am testing in a wrong way.\nDefinitions\nI/O operation : Any process than does not consume CPU cycle. Calling a database, making http calls over network etc.\nCPU operations: Any process or operation that consumer CPU cycle. Password Encryption, data manipulation, sorting, filtering, merge. Basically any data manipulation with data structure.\nSystem Config and versions\nOS: Ubuntu 22.04.1 LTS 64 Bit\nMemory: 32 GB\nProcessor: 12th Gen Intel® Core™ i7–1270P\nCores: 16\nNode version : v19.3.0\nSpring boot version : 3.0.2-SNAPSHOT\nJava version : 18.0.2-ea\nPhase I (String return)\nCreating a server on node using express by copying the code from official website that will return “Hello World!”, same step with spring-boot as well. Testing the performance using Apache ab with different concurrency and requests\nObservations : Nodejs was consistent in ab results, on the other hand java took time to lauch thread at first run but once the thread connections are established it becomes consistent.\nRunning Process: Ran Nodejs express server using pm2 for utilizing all the cores, java doesn’t need any thing for it.\nBelow are the results\nNODEJS\n❯ ab -n 1000000 -c 1000 &quot;http://localhost:3000/&quot;  \nThis is ApacheBench, Version 2.3 &lt;$Revision: 1879490 $&gt;  \nCopyright 1996 Adam Twiss, Zeus Technology Ltd, www.zeustech.net/  \nLicensed to The Apache Software Foundation, www.apache.org/  \n  \nBenchmarking localhost (be patient)  \nCompleted 100000 requests  \nCompleted 200000 requests  \nCompleted 300000 requests  \nCompleted 400000 requests  \nCompleted 500000 requests  \nCompleted 600000 requests  \nCompleted 700000 requests  \nCompleted 800000 requests  \nCompleted 900000 requests  \nCompleted 1000000 requests  \nFinished 1000000 requests  \n  \n  \nServer Software:          \nServer Hostname:        localhost  \nServer Port:            3000  \n  \nDocument Path:          /  \nDocument Length:        12 bytes  \n  \nConcurrency Level:      1000  \nTime taken for tests:   46.510 seconds  \nComplete requests:      1000000  \nFailed requests:        0  \nTotal transferred:      211000000 bytes  \nHTML transferred:       12000000 bytes  \nRequests per second:    21500.80 [#/sec] (mean)  \nTime per request:       46.510 [ms] (mean)  \nTime per request:       0.047 [ms] (mean, across all concurrent requests)  \nTransfer rate:          4430.34 [Kbytes/sec] received  \n  \nConnection Times (ms)  \n              min  mean[+/-sd] median   max  \nConnect:        0   11   7.4     12      38  \nProcessing:    13   35  14.8     31      92  \nWaiting:        1   31  15.4     28      88  \nTotal:         25   46  11.1     43      92  \n  \nPercentage of the requests served within a certain time (ms)  \n  50%     43  \n  66%     48  \n  75%     54  \n  80%     58  \n  90%     64  \n  95%     67  \n  98%     72  \n  99%     77  \n 100%     92 (longest request)  \n\nJava\n❯ ab -n 1000000 -c 1000 &quot;http://localhost:8080/&quot;  \nThis is ApacheBench, Version 2.3 &lt;$Revision: 1879490 $&gt;  \nCopyright 1996 Adam Twiss, Zeus Technology Ltd, www.zeustech.net/  \nLicensed to The Apache Software Foundation, www.apache.org/  \n  \nBenchmarking localhost (be patient)  \nCompleted 100000 requests  \nCompleted 200000 requests  \nCompleted 300000 requests  \nCompleted 400000 requests  \nCompleted 500000 requests  \nCompleted 600000 requests  \nCompleted 700000 requests  \nCompleted 800000 requests  \nCompleted 900000 requests  \nCompleted 1000000 requests  \nFinished 1000000 requests  \n  \n  \nServer Software:          \nServer Hostname:        localhost  \nServer Port:            8080  \n  \nDocument Path:          /  \nDocument Length:        12 bytes  \n  \nConcurrency Level:      1000  \nTime taken for tests:   25.762 seconds  \nComplete requests:      1000000  \nFailed requests:        0  \nTotal transferred:      91000000 bytes  \nHTML transferred:       12000000 bytes  \nRequests per second:    38816.91 [#/sec] (mean)  \nTime per request:       25.762 [ms] (mean)  \nTime per request:       0.026 [ms] (mean, across all concurrent requests)  \nTransfer rate:          3449.55 [Kbytes/sec] received  \n  \nConnection Times (ms)  \n              min  mean[+/-sd] median   max  \nConnect:        0   12   1.1     12      22  \nProcessing:     4   14   2.3     13      29  \nWaiting:        1   10   2.1     10      26  \nTotal:         12   26   2.2     25      42  \n  \nPercentage of the requests served within a certain time (ms)  \n  50%     25  \n  66%     26  \n  75%     27  \n  80%     28  \n  90%     29  \n  95%     30  \n  98%     31  \n  99%     32  \n 100%     42 (longest request)\n\nNode.js Code:\nconst express = require(&#039;express&#039;)  \nconst app = express()  \nconst port = 3000  \n  \napp.get(&#039;/&#039;, (req, res) =&gt; {  \n    \n  res.send(&#039;Hello World!&#039;)  \n})  \n  \napp.listen(port, () =&gt; {  \n  console.log(`Example app listening on port ${port}`)  \n})\n\nJava Code:\npackage com.prakritidev.verma.javasever;  \n  \nimport org.springframework.boot.SpringApplication;  \nimport org.springframework.boot.autoconfigure.SpringBootApplication;  \nimport org.springframework.web.bind.annotation.GetMapping;  \nimport org.springframework.web.bind.annotation.RequestMapping;  \nimport org.springframework.web.bind.annotation.RestController;  \n  \n@SpringBootApplication  \npublic class JavaseverApplication {  \n  \n public static void main(String[] args) {  \n  SpringApplication.run(JavaseverApplication.class, args);  \n }  \n  \n}  \n  \n@RestController  \n@RequestMapping  \nclass Controller {   \n   \n @GetMapping(path = &quot;/&quot;)  \n public String helloWorld() {  \n  return &quot;Hello World!&quot;;  \n }  \n  \n}\n\nPhase 2 : Simple Hashing implementation (CPU Task)\nCode reference: stackoverflow.com/questions/7616461/generate-a-hash-from-string-in-javascript\nBelow are the results:\nPhase 2 (Hashing Implementation)\nI was not expecting this result. will do some some more testing in this phase.\nJava\n❯ ab -n 1000000 -c 1000 &quot;http://localhost:8080/?input=duck&quot;  \nThis is ApacheBench, Version 2.3 &lt;$Revision: 1879490 $&gt;  \nCopyright 1996 Adam Twiss, Zeus Technology Ltd, www.zeustech.net/  \nLicensed to The Apache Software Foundation, www.apache.org/  \n  \nBenchmarking localhost (be patient)  \nCompleted 100000 requests  \nCompleted 200000 requests  \nCompleted 300000 requests  \nCompleted 400000 requests  \nCompleted 500000 requests  \nCompleted 600000 requests  \nCompleted 700000 requests  \nCompleted 800000 requests  \nCompleted 900000 requests  \nCompleted 1000000 requests  \nFinished 1000000 requests  \n  \n  \nServer Software:          \nServer Hostname:        localhost  \nServer Port:            8080  \n  \nDocument Path:          /?input=duck  \nDocument Length:        7 bytes  \n  \nConcurrency Level:      1000  \nTime taken for tests:   38.361 seconds  \nComplete requests:      1000000  \nFailed requests:        0  \nTotal transferred:      85000000 bytes  \nHTML transferred:       7000000 bytes  \nRequests per second:    26067.86 [#/sec] (mean)  \nTime per request:       38.361 [ms] (mean)  \nTime per request:       0.038 [ms] (mean, across all concurrent requests)  \nTransfer rate:          2163.84 [Kbytes/sec] received  \n  \nConnection Times (ms)  \n              min  mean[+/-sd] median   max  \nConnect:        0   18   2.3     18      34  \nProcessing:     8   20   2.5     20      45  \nWaiting:        1   14   2.5     14      38  \nTotal:         18   38   1.7     38      62  \n  \nPercentage of the requests served within a certain time (ms)  \n  50%     38  \n  66%     38  \n  75%     39  \n  80%     39  \n  90%     40  \n  95%     41  \n  98%     43  \n  99%     45  \n 100%     62 (longest request)  \n\nNodejs\n ab -n 1000000 -c 1000 &quot;http://localhost:3000/?input=duck&quot;  \nThis is ApacheBench, Version 2.3 &lt;$Revision: 1879490 $&gt;  \nCopyright 1996 Adam Twiss, Zeus Technology Ltd, www.zeustech.net/  \nLicensed to The Apache Software Foundation, www.apache.org/  \n  \nBenchmarking localhost (be patient)  \nCompleted 100000 requests  \nCompleted 200000 requests  \nCompleted 300000 requests  \nCompleted 400000 requests  \nCompleted 500000 requests  \nCompleted 600000 requests  \nCompleted 700000 requests  \nCompleted 800000 requests  \nCompleted 900000 requests  \nCompleted 1000000 requests  \nFinished 1000000 requests  \n  \n  \nServer Software:          \nServer Hostname:        localhost  \nServer Port:            3000  \n  \nDocument Path:          /?input=duck  \nDocument Length:        7 bytes  \n  \nConcurrency Level:      1000  \nTime taken for tests:   105.177 seconds  \nComplete requests:      1000000  \nFailed requests:        0  \nTotal transferred:      205000000 bytes  \nHTML transferred:       7000000 bytes  \nRequests per second:    9507.81 [#/sec] (mean)  \nTime per request:       105.177 [ms] (mean)  \nTime per request:       0.105 [ms] (mean, across all concurrent requests)  \nTransfer rate:          1903.42 [Kbytes/sec] received  \n  \nConnection Times (ms)  \n              min  mean[+/-sd] median   max  \nConnect:        0    2   4.7      0      36  \nProcessing:    12  103  21.3     99     238  \nWaiting:        5  102  21.9     99     237  \nTotal:         26  105  19.8    100     238  \n  \nPercentage of the requests served within a certain time (ms)  \n  50%    100  \n  66%    103  \n  75%    105  \n  80%    107  \n  90%    113  \n  95%    139  \n  98%    194  \n  99%    209  \n 100%    238 (longest request)\n\nNode.js Code:\nconst express = require(&#039;express&#039;)  \nconst app = express()  \nconst port = 3000  \n  \n  \napp.get(&#039;/&#039;, (req, res) =&gt; {  \n  const { input } = req.query  \n  let hash = 0,  \n  i, chr;  \n  if (input.length === 0) return 0;  \n  for (i = 0; i &lt; input.length; i++) {  \n    chr = input.charCodeAt(i);  \n    hash = ((hash &lt;&lt; 5) - hash) + chr;  \n    hash |= 0; // Convert to 32bit integer  \n  }  \n    \n    \n  console.log(String(input), hash);  \n  res.send(String(hash), 200);  \n})  \n  \napp.listen(port, () =&gt; {  \n  console.log(`Example app listening on port ${port}`)  \n})\n\nJava Code :\npackage com.prakritidev.verma.javasever;  \n  \nimport org.springframework.boot.SpringApplication;  \nimport org.springframework.boot.autoconfigure.SpringBootApplication;  \nimport org.springframework.web.bind.annotation.GetMapping;  \nimport org.springframework.web.bind.annotation.RequestMapping;  \nimport org.springframework.web.bind.annotation.RequestParam;  \nimport org.springframework.web.bind.annotation.RestController;  \n  \n@SpringBootApplication  \npublic class JavaseverApplication {  \n  \n public static void main(String[] args) {  \n  SpringApplication.run(JavaseverApplication.class, args);  \n }  \n  \n}  \n  \n@RestController  \n@RequestMapping  \nclass Controller {  \n  \n @GetMapping(path = &quot;/&quot;)  \n public String helloWorld(@RequestParam String input) {  \n  var hash = 0;   \n  var i = 0;  \n  char chr;  \n    if (input.length() == 0) return &quot;0&quot;;  \n  for (i = 0; i &lt; input.length(); i++) {  \n   chr = input.charAt(i);  \n   hash = ((hash &lt;&lt; 5) - hash) + chr;  \n   hash |= 0; // Convert to 32bit integer  \n  }  \n  \n  return String.valueOf(hash);  \n }  \n  \n}\n\nPhase 3 : Redis set and fetch call (I/O)\n… working on it\nPhase 4 : Mysql insert and fetch call (I/O)\n… working on it\nPhase 5: Websocket implementation (I/O)\n… working on it\nPhase 6: Backpressure implemenration\n… working on it"},"Tech/Bloom-Filter-Implementation-in-Java-":{"title":"\"Bloom Filter Implementation in Java \"","links":[],"tags":["java","probabilistic","Algorithm","datastructure"],"content":""},"Tech/Caching":{"title":"\"Caching\"","links":[],"tags":[],"content":"Computer\n\nL1\n\nSmallest Cache resides inside CPU # kbs\n\n\nL2\n\nOn CPU # MBs\n\n\nL3\n\nSlowest of them all, shared by multiple CPUs cores\n\n\n"},"Tech/Cloud-Design-Patterns":{"title":"\"Cloud Design Patterns\"","links":[],"tags":["architecture","autoscaling","DesignPrinciples","distributed"],"content":"Cloud Design Patterns\nScalability : \nArchitecture that scale in a linear manner where adding extra resources results in at least a proportional increase in ability to serve additional load**.** There are generally two ways to scale an IT architecture: vertically and horizontally\nScaling Vertically :  \nIncreasing the hardware capacity by stopping the instance and increase the capacity, Good for short term scaling. Not a good way to scaling for long shot because It has its limits and not always cost effective and reliable. \nHorizontal Scaling :\nIncrease the number of resources, but not all architecture are capable of horizontal scaling or to distribute their workload to multiple resources. Bellow are some scenarios fro horizontal scaling. \nStateless Applications\n\nA stateless application is an application that needs no knowledge of previous interactions and stores no session information. \n\nHow to distribute load to multiple nodes\n\nPush model: Use load balancers or Route53(does not help every time).\nPull model: Asynchronous event-driven workloads.\n\nStatefull Applications: \n\nDatabased are sate-full\n\nDistributed Processing : \n\nHadoop, AWS EMR.\n"},"Tech/Efficient-Data-Processing-on-a-Budget---My-Journey-with-File-Based-Queues":{"title":"Efficient Data Processing on a Budget - My Journey with File-Based Queues","links":[],"tags":["python","architecture","parallelProcessing","dataProcessing","queue","multithreading","multiprocessing","AsyncCodes"],"content":"I want to share my experience with managing background tasks in a Python application, especially in the context of financial data processing. This isn’t just about choosing between file-based queues and Celery, but also about making decisions under hardware constraints and specific industry requirements.\nThe Challenge\nMy task was to process intraday data for over 4,000 equities — a hefty amount of data! All this needed to be efficiently inserted into MongoDB, which was hosted on a modest server with just 1 core and 2 GB RAM, also running Docker. The catch? I had to do this within a tight budget and resource constraints.\nWhy I Chose File-Based Queues?\nGiven the server limitations, I decided against using Celery, despite its robust features. Here’s why:\n\nHardware Limitations: My server setup was quite basic — definitely not cut out for the additional load that comes with Celery and a message broker like Redis or RabbitMQ.\nBudget Constraints: As a small-scale operation in the finance sector, keeping costs low was crucial. More complex setups would mean more resources and potentially higher costs.\nSimplicity and Reliability: I needed something straightforward yet reliable. File-based queues allowed me to manage tasks effectively without overcomplicating the system.\n\nThe Setup and Results: I set up a file-based queue system with two worker threads using Futures. This approach was surprisingly efficient. The entire process of handling the intraday data for over 4,000 equities (Roughly 7 Lakh document processing) and inserting them into MongoDB took about 2.5 hours. Not bad, right? Especially considering the server constraints!\nHere’s a Peek at the Code: To give you a better idea, here’s a simplified example of the file-based queue system I used:\nimport json\nimport concurrent.futures\nSample task file (tasks.json)\n[  \n{&quot;task_id&quot;: 1, &quot;data&quot;: &quot;Task 1 data&quot;},  \n{&quot;task_id&quot;: 2, &quot;data&quot;: &quot;Task 2 data&quot;},  \n ...  \n]\n\ndef process_task(task):  \n    print(f&quot;Processing task {task[&#039;task_id&#039;]}: {task[&#039;data&#039;]}&quot;)  \n    # Do Some processing or storing it to DB etc etc.   \n  \ndef main():  \n    try:  \n        with open(&#039;tasks.json&#039;, &#039;r&#039;) as file:  \n            tasks = json.load(file)  \n        # Sadly I can&#039;t increase the workers due to hardware limition.  \n        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:  \n            future_to_task = {executor.submit(process_task, task): task for task in tasks}  \n  \n            for future in concurrent.futures.as_completed(future_to_task):  \n                task = future_to_task[future]  \n                try:  \n                    future.result()  \n                except Exception as e:  \n                    print(f&quot;Task {task[&#039;task_id&#039;]} generated an exception: {e}&quot;)  \n                else:  \n                    print(f&quot;Task {task[&#039;task_id&#039;]} completed successfully.&quot;)  \n  \n        print(&quot;All tasks completed.&quot;)  \n  \n    except Exception as e:  \n        print(f&quot;An error occurred when reading the task file: {e}&quot;)  \n  \nif __name__ == &quot;__main__&quot;:  \n    main()\n\nWhen Would I Consider Celery?\nNow, Celery is an excellent tool, no doubt. It shines in scenarios where you have:\n\nMore Complex Task Management: If you’re juggling a multitude of tasks that need advanced routing or scheduling.\nHigher Scalability Needs: For applications that need to scale and distribute tasks across multiple servers.\nResource Availability: If you have a server setup that can handle the extra load without breaking a sweat (or the bank!).\n\nMy journey with file-based queues has been a testament to the fact that sometimes, simplicity and resourcefulness trump complexity, especially in the world of finance where every penny and every millisecond counts. Whether you’re a fellow data enthusiast in finance or just someone exploring background task management in Python, I hope my experience sheds some light on the practical aspects of choosing the right tool for the right job!"},"Tech/GNU-Parallel---Alternative-to-Language-Programming":{"title":"\"GNU Parallel - Alternative to Language Programming\"","links":[],"tags":["GNU","dataProcessing","parallelProcessing"],"content":""},"Tech/Git-Advanced":{"title":"\"Git Advanced\"","links":[],"tags":["Git"],"content":""},"Tech/Git-Basics":{"title":"\"Introduction\"","links":[],"tags":["Git","github"],"content":"What is Git ?\nOfficial Definition is its a distributed version control. To those who are in college or maybe in school or maybe who is just starting in tech, the eaiser way to understand it. Its a tool where it can store multiple versions of your files or a single file. It maintains the history for all the changes you have done. Not only that it can also keep different versions of you file exist in same directory. It gives us the power of looking back into the past and keep the directory clean.\nSo, its better to use git rather than naming the you files like “final.txt”, “final1.txt”, “final_final.txt”\nIn order to get that ability to look into the past. you have you create a folder or directory where you need this power.\nOfcourse you need to install it first. Just look at any installation videos or ask chatGPT for it.\nGit init\ngit init\nopen the terminal in the folder you created. or maybe cd to the directory you created and run the above command.\nThis will initialise the a git repository and a hidden .git folder is created in it.\nGit add\nGit Commit\n\n\n[!www.gogle.com] Title\nThis is my content\n"},"Tech/Index":{"title":"Index","links":[],"tags":[],"content":""},"Tech/JQ-(Basics)---Json-Processing-on-terminal":{"title":"\"JQ (Basics) - Json Processing on terminal\"","links":[],"tags":["terminal","json","jq","basics","easy","dataProcessing"],"content":"What is jq ?\nWhy this is better than python or js ?"},"Tech/Java---Count-Min-Sketch-":{"title":"\"Java - Count Min Sketch \"","links":[],"tags":["java","Algorithm","basics","datastructure","probabilistic"],"content":""},"Tech/Java---Multi-Threading-Why-":{"title":"\"Java - Multi Threading Why ?\"","links":[],"tags":["java","dataProcessing","Performance","Algorithm"],"content":""},"Tech/Java---jdk-(open-source-getting-started)":{"title":"\"Java - jdk (open source getting started)\"","links":[],"tags":["java","jdk","opensource"],"content":""},"Tech/Java-Streams-(Part-1)":{"title":"\"Java - Streams\"","links":[],"tags":["java","streams","functionalprograming","parallelProcessing"],"content":""},"Tech/MySQL-8-Write-Ahead-Async-Log-":{"title":"MySQL 8 Write Ahead Async Log","links":[],"tags":["MySQL","WAL","Async"],"content":""},"Tech/Terminal---Warp-vs-iterm2":{"title":"\"Terminal - Warp vs iterm2\"","links":[],"tags":["terminal","warp","iterm2"],"content":""},"Tech/Transations---2-PC-and-3-PC-":{"title":"\"Transations - 2 PC and 3 PC \"","links":[],"tags":["database","transactions","distributed","Algorithm"],"content":""},"Tech/What-is-IPC-":{"title":"\"What is IPC ?\"","links":[],"tags":[],"content":""},"Tech/What-is-a-Socket-":{"title":"\"What is a Socket ?\"","links":[],"tags":["socket"],"content":""},"Tech/What-is-gRPC-":{"title":"\"What is gRPC? \"","links":[],"tags":[],"content":""},"Understanding-Of-Life/How-to-earn-respect-in-front-of-parents--index":{"title":"\"How to earn respect in front of parents ? \"","links":[],"tags":["life","philosophy","parents","respect","negotiation"],"content":"Although it’s a very difficult topic to pick on, over the last decade I have seen some patterns that seemed very common amongst the parents and how they treat their children. \nThey say “Respect has to be earned”"},"Understanding-Of-Life/Index":{"title":"Index","links":[],"tags":[],"content":""},"Understanding-Of-Life/What-is-the-essence-of-this-series":{"title":"\"What is the essence of this series?\"","links":[],"tags":["life","philosophy"],"content":"Over a considerable span of time, I’ve come to grasp certain insights that prove elusive when articulated in words alone.\nThese notions often defy direct explanation; there isn’t a single book that encapsulates them neatly. You may read a hundred books spanning philosophy, poetry, and art in pursuit of comprehension. However, amidst this voluminous sea of wisdom, one risks becoming lost and intoxicated by the knowledge amassed.\nAt first glance, this may sound somewhat conceited, but it appears that I’ve managed to glean some understanding of life’s intricacies. Of course, the definition of life’s essence can differ greatly from person to person. My hope is that upon revisiting these thoughts or for anyone else encountering them, they will resonate in a deeply personal and meaningful way.\nWith this series, let’s embark on an exploration of captivating topics and intriguing characters, particularly those who have piqued my curiosity within the confines of my workplace."},"index":{"title":"Data & Code Chronicles","links":[],"tags":[],"content":"Yohohohohoho (OnePiece reference)!, tech enthusiasts! Welcome to my blog, where we dive into the exciting and ever-evolving world of data engineering, machine learning, and system architecture. I’m Prakritidev Verma, and I’ve spent my career tackling the complex challenges of the tech landscape.\nWhat You’ll Find Here\nIn this space, I share my experiences, insights, and tips on a variety of topics that I’m passionate about:\n\nProgramming Mastery: From Java to Python, Go, Lua, and JavaScript, I’ll share the tricks and best practices I’ve picked up along the way.\nMachine Learning Adventures: Whether it’s traditional ML with Scikit-learn and TensorFlow or specialized areas like Financial Machine Learning, I’ll break down the concepts and applications.\nData Engineering: Get insights into tools like ElasticSearch, Neo4j, and MongoDB, and learn how to build robust data pipelines with frameworks like Spring Boot, Kafka, and the Elastic Stack (ELK).\nCloud and DevOps: Dive into the world of AWS, S3, EC2, SQS, and CloudFoundry. Plus, I’ll share my favorite scripting tools like Shell Script, GNU Parallel, jq, and vim to streamline your workflow.\nReal-World Applications: I’ll discuss how these technologies come together in real-world projects, from handling millions of data events per day to optimizing search engines and creating scalable, efficient systems.\n\nMy Journey\nI’ve led projects that manage massive data ingestion pipelines, crafted user-centric ranking algorithms, and built real-time analytics solutions. My work has spanned various domains, including email campaign analytics, job search engines, and financial market analysis. Along the way, I’ve learned the value of choosing the right tool for the job, balancing complexity with performance, and always pushing the envelope of what’s possible.\nWhy Follow This Blog?\nIf you’re as excited about tech as I am, you’re in the right place. Whether you’re a seasoned developer or just starting out, my goal is to provide you with valuable insights, practical tips, and a glimpse into the cutting-edge technologies shaping our future. Let’s explore, learn, and innovate together.\nTopics We’ll Explore Together\n\nData Pipelines and Streaming Data: Discover the ins and outs of setting up efficient data pipelines, handling streaming data, and ensuring real-time analytics.\nGraph Databases: Learn how to leverage Neo4j and other graph databases for complex relationship mapping and data lineage.\nSearch and Discoverability: Dive into the mechanics of search engines, with a focus on ElasticSearch and how to optimize it for large-scale data retrieval.\nScalable Architectures: Get a deep understanding of building scalable systems that can handle millions of requests per day without breaking a sweat.\nDevOps Best Practices: From setting up CI/CD pipelines to managing cloud infrastructure, I’ll share the tools and techniques that keep everything running smoothly.\n\nConnect ?\nYou can also connect with me on LinkedIn and check out my projects on GitHub.\nStick around, and let’s geek out over the amazing possibilities of technology!"}}